[["index.html", "Bayes Rules! Book Club Welcome", " Bayes Rules! Book Club The R4DS Online Learning Community 2022-06-18 Welcome Welcome to the bookclub! This is a companion for the book Bayes Rules! by Alicia A. Johnson, Miles Q. Ott, and Mine Dogucu (Chapman and Hall/CRC, copyright 2022, 9780367255398). This companion is available at r4ds.io/bayes_rules. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. Following the flow! Source: https://www.youtube.com/watch?v=zYYBtxHWE0A From: Richard McElreath, Statistical Rethinking videos "],["preface.html", "Preface ", " Preface "],["bayesian-statistics.html", "0.1 Bayesian statistics?", " 0.1 Bayesian statistics? Frequentist and Bayesian methods share: learning from data But Bayesian allows: new data + prior results easier to interpret shines when frequentist fails computational tools more accesible now "],["tips-and-tricks-from-the-authors.html", "0.2 Tips and tricks from the authors", " 0.2 Tips and tricks from the authors Learn by doing Embrace a growth mindset (we will do mistakes!) Interpret Bayes in a context (ethics and maybe more) Practice, practice, practice "],["set-up.html", "0.3 Set up", " 0.3 Set up Install rstan : https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started install.packages(c(&quot;bayesrules&quot;, &quot;tidyverse&quot;, &quot;janitor&quot;, &quot;rstanarm&quot;, &quot;bayesplot&quot;, &quot;tidybayes&quot;, &quot;broom.mixed&quot;, &quot;modelr&quot;, &quot;e1071&quot;, &quot;forcats&quot;), dependencies = TRUE) On linux (ubuntu 22) I had to update some dependencies. "],["the-authors.html", "0.4 The authors:", " 0.4 The authors: Alicia A. Johnson : Website https://ajohns24.github.io/portfolio/ Miles Q. Ott: https://twitter.com/Miles_Ott Mine Dogucu: https://twitter.com/MineDogucu "],["the-big-bayesian-picture.html", "Chapter 1 The Big (Bayesian) Picture", " Chapter 1 The Big (Bayesian) Picture Learning objectives: Learn to think like a Bayesian. Explore the foundations of a Bayesian data analysis and how they contrast with the frequentist alternative Learn a little bit about the history of the Bayesian philosophy "],["thinking-like-a-bayesian-14.html", "1.1 Thinking like a Bayesian 1/4", " 1.1 Thinking like a Bayesian 1/4 DiagrammeR::grViz(&quot; digraph thinking_bayesian{ # node statement node [shape = oval] a [label = &#39;Prior&#39;]; b [label = &#39;Data&#39;]; c [label = &#39;Posterior&#39;]; d [label = &#39;New data&#39;]; e [label = &#39;Posterior&#39;]; f [label = &#39;New data&#39;] g [style = invisible ] # edge statement a -&gt; c b -&gt; c c -&gt; e d -&gt; e f-&gt; g [style = dashed] e-&gt; g [style = dashed] }&quot;) Figure 1.1: A Bayesian knowledge-building diagram Both Bayesian and frequentist share a common goal: learn from data about the world around Both use data to fit nodels, make predictions and evaluate hypothesis "],["quiz-time.html", "1.2 Quiz time!", " 1.2 Quiz time! 4-5: frequentist 6-8: a bit of both 9-12: Bayesian TODO link to script and data in repo "],["thinking-like-a-bayesian-24.html", "1.3 Thinking like a Bayesian 2/4", " 1.3 Thinking like a Bayesian 2/4 1.3.1 Interpreting probability: Bayesian philosophy: relative plausibility of an event Frequentist philosophy: long-run relative frequency of a repeatable event "],["thinking-like-a-bayesian-34.html", "1.4 Thinking like a Bayesian 3/4", " 1.4 Thinking like a Bayesian 3/4 1.4.1 Bayesian balancing act Two claims: Zuofo claims he can predict the outcome of coin flip Kavya claims she can distinguish between natural and artificial sweeteners If both succeed with a 10/10 sucess rate what can we conclude from this? The frequentist approach will discard prior knowledge (it is harder to predict coin flip that having a sensitive palate to sweeteners) and the Bayesian want to use this prior knowledge. -&gt; How can we balance Prior and Data? "],["thinking-like-a-bayesian-44.html", "1.5 Thinking like a Bayesian 4/4", " 1.5 Thinking like a Bayesian 4/4 1.5.1 Asking question What’s the chance that I actually have the disease (a)? Versus I do not have the disease, What’s the chance that I would have gotten this positive test results (b)? # building data disease &lt;- c(rep(&quot;disease&quot;, 4), rep(&quot;no disease&quot;, 96)) a &lt;- &quot;test positive&quot; ; b &lt;- &quot;test negative&quot; test &lt;- c(rep(a, 3), b, rep(a, 9), rep(b, 87)) disease_status &lt;- data.frame(disease, test) # contingency table contingency_disease &lt;- table(disease_status) contingency_disease &lt;- addmargins(contingency_disease) knitr::kable(contingency_disease ) test negative test positive Sum disease 1 3 4 no disease 87 9 96 Sum 88 12 100 (a): 3 / 12 (b): 9 / 96 Analogy between (b) and p-value: it is more natural to study the uncertainty of a yet-unproven hypothesis than the uncertainty of data we have already observed.(authors’opinion) "],["quick-history-lesson.html", "1.6 Quick history lesson", " 1.6 Quick history lesson From stigmatized to being used in modeling COVID-19 rates. Why? advances in computing departure from tradition (what people learn is what people use) reevaluation of subjectivity : frequentist is also subjective and subjectivity is not any more a dirty word. "],["look-ahead.html", "1.7 Look ahead", " 1.7 Look ahead 1.7.1 4 units Bayesian foundations: 5 chapters Focus: models &amp; distributions (conjugate family) Posterior simulations &amp; analysis: 3 chapters Focus: when conjugate is not an option: MCMC then posterior analysis Bayesian regression &amp; classification Focus: extending unit 1 reponse variable (Y) with predictor variables (X) Hierarchical Bayesian models Focus: expanding unit 3 to accomodate and harness grouped data. "],["summary.html", "1.8 Summary", " 1.8 Summary Posterior knowledge &lt;- balancing information from data and prior knowledge More “waves”of data -&gt; refine knowledge (less effect of prior) With more and more data, two analysts will converge on the same posterior knowledge "],["resources-mentioned.html", "1.9 Resources mentioned", " 1.9 Resources mentioned This is a list of resources mentioned in the first meeting! 1.9.1 Other Bayesian books: Richard McElreath * book: https://xcelab.net/rm/statistical-rethinking/ * vidéo: https://github.com/rmcelreath/stat_rethinking_2022 The “puppy” book by John K. Kruschke : https://sites.google.com/site/doingbayesiandataanalysis/ Introduction to Bayesian Thinking, Clyde, Centinkaya-Rundel et al similar level to our book Bayesian Data Analysis, Andrew Gelman more precise (and mathematical) Intro to Bayes Theorem, Wrath of Math, video Clear explanation of the meaning of given in statements like probability of A given B. $ P(A | B) $ 1.9.2 Drawing DAG (Directed Acyclic Graph) Pen and paper DiagrammeR: for drawing diagram, uses Graphviz or mermaid Dagitty: for causal diagrams 1.9.3 Podcast Learning Bayesian Statistics ep. 42 With Mine Dogucu "],["meeting-videos.html", "1.10 Meeting Videos", " 1.10 Meeting Videos 1.10.1 Cohort 1 Meeting chat log LOG "],["bayes-rule.html", "Chapter 2 Bayes’ Rule", " Chapter 2 Bayes’ Rule Learning objectives: Explore foundational probability tools conditional probability: Probability of A given B \\(P(A|B)\\) joint probability: Probability of A and B occuring \\(P(A \\cap B)\\) marginal probability: Probability of an event \\(P(A)\\) Law of Total Probability: If a probability of an event is unknown it can be calculated using the know probability of other related event Conduct first formal Bayesian analysis Practice your Bayesian grammar Prior Likelihood Normalizing constant Simulate Bayesian models sample() sample_n() rbinon() "],["building-a-bayesian-model-for-events.html", "2.1 Building a Bayesian model for events", " 2.1 Building a Bayesian model for events First Data set ?? fake_news Figure 2.1: Bayesian knowledge-building diagram for wether or not the article is fake Two variables: - fake vs real - ! or not 2.1.1 Workflow: Prior probability model A model for interpreting the data Posterior probability model \\[ P(FakeNew = 0.4 ) \\quad and \\quad P(Real = 0.6) \\] \\[ P(B = 0.4 ) \\quad and \\quad P(B^c = 0.6) \\] Here \\(P(FakeNew)\\) : prior probability of an article to be a fake news Valid probability model : 1. accounts all event 2. assign probabilities for each event 3. sum to one \\(P(ExClam)\\) : probability that an article contains an exclamation mark in his title We know that if an article is fake news : 26.67% that the title contains ! and if it is not fake this is just 2.22%. \\[ P(Exclam|FakeNew) = 0.2667 \\quad and \\quad P(Exclam|Real = 0.0222) \\] This is a conditional probability. Conditional probability help know if B give us insight in A. If it does not provide any information it means that A and B event are independent (\\(P(A|B) = P(A)\\)). "],["normalizing-constant.html", "2.2 Normalizing constant", " 2.2 Normalizing constant \\(P(Exclam|FakeNew) = 0.2667 \\quad and \\quad P(Exclam|Real) = 0.0222)\\) are our likelihood, when we know A (!) we know that getting B (Fake news) is more likely. It is different that our prior probability. Then we need can calculate the joint probability (probability of observing A nd B for example), of each options (here it is half of them): \\[ P(Exclam \\cap FakeNew) = P(Exclam|FakeNew) P (Fakenew) = 0.2667 *0.4 = 0.1067 \\] \\[ P(PasExclam \\cap FakeNew ) = (1 - P(Exclam|FakeNew)) * P(FakeNew)) = (1 - 0.2667) * 0.4 = 0.2993 \\] Here \\(P(B)\\) is the marginal probability of B B &lt;- c(0.1067, 0.2933, 0.4) Bc &lt;- c(0.0133, 0.5867, 0.6) Total &lt;- c(0.12, 0.88, 1) joint_p &lt;- data.frame(B, Bc, Total, row.names = c(&quot;A&quot;, &quot;Ac&quot;, &quot;Total&quot;)) knitr::kable(joint_p) B Bc Total A 0.1067 0.0133 0.12 Ac 0.2933 0.5867 0.88 Total 0.4000 0.6000 1.00 \\(P(Exclam)\\) is our normalizing constant. Ok but we want \\(P(FakeNew|exlam)\\) ie \\(P(B|A)\\) \\[ P(FakeNew|exclam) = \\frac{P(exclam \\cap FakeNew)}{P(Exclam)} = \\frac{P(FakeNew)L(FakeNew|Exclam)}{P(Exclam)} \\] \\[ posterior = \\frac{prior . likelihood}{normalizing \\quad constant} = \\frac{0.4 * 0.2667}{0.12} = 0.889 \\] "],["posterior-simulation.html", "2.3 Posterior simulation", " 2.3 Posterior simulation This was a model. Now we will use a simulation! library(dplyr) library(ggplot2) set.seed(84735) # Define possible articles article &lt;- data.frame(type = c(&quot;real&quot;, &quot;fake&quot;)) # Define the prior model prior &lt;- c(0.6, 0.4) article_sim &lt;- dplyr::sample_n(article, size = 10000, weight = prior, replace = TRUE) # dats model article_sim &lt;- article_sim %&gt;% mutate(data_model = case_when(type == &quot;fake&quot; ~ 0.2667, type == &quot;real&quot; ~ 0.0222)) # Simulate exclamation point usage data &lt;- c(&quot;NoExclam&quot;, &quot;Exclam&quot;) set.seed(3) # Rbase simplier ? article_sim &lt;- article_sim %&gt;% group_by(1:n()) %&gt;% mutate(usage = sample(data, size = 1, prob = c(1 - data_model, data_model))) ggplot(article_sim, aes(x = type)) + geom_bar() + facet_wrap(~ usage) "],["example-pop-vs-soda-vs-coke.html", "2.4 Example Pop vs Soda vs Coke", " 2.4 Example Pop vs Soda vs Coke Expend TRUE/FALSE example with one with categories. "],["building-a-bayesian-model-for-random-variables-1n.html", "2.5 Building a Bayesian model for random variables (1/n)", " 2.5 Building a Bayesian model for random variables (1/n) 2.5.1 First step prior \\(\\pi\\) : skill of Kasparov relative to Deep Blue (random variable) Prior model of \\(\\pi\\): pi &lt;- c(0.2, 0.5, 0.8, &quot;total&quot;) # pmf : probability mass functions pmf &lt;- c(0.1, 0.25, 0.65, 1) prior_pi &lt;- data.frame(pi, pmf) knitr::kable(t(prior_pi)) pi 0.2 0.5 0.8 total pmf 0.10 0.25 0.65 1.00 "],["building-a-bayesian-model-for-random-variables-1n-1.html", "2.6 Building a Bayesian model for random variables (1/n)", " 2.6 Building a Bayesian model for random variables (1/n) 2.6.1 Binomial data model Y is the number of games (on 6 games) that Kasparov wins. Y our random variable: {0, 1, …., 6} , depends on \\(\\pi\\) \\[f(y|\\pi) = P (Y = y|\\pi ) \\] \\(y\\) : any possible outcone If we assume games are independents (no effect on each other) and \\(\\pi\\) is fixed we can use the Binomial model. Y is the number of successes in a fixed number of trials (\\(n\\)) \\[ Y|\\pi \\sim Bin(n, \\pi) \\] \\[ f(y|\\pi) = \\begin{pmatrix} 6 \\\\y \\end{pmatrix} \\pi^y(1 - \\pi)^{6 - y} \\quad for \\quad y \\in \\begin{Bmatrix} 0, 1, 2, 3, 4, 5, 6 \\end{Bmatrix} \\] We can use the prior for \\(\\pi\\) and all \\(y\\) to calculate each probabilities. "],["building-a-bayesian-model-for-random-variables-1n-2.html", "2.7 Building a Bayesian model for random variables (1/n)", " 2.7 Building a Bayesian model for random variables (1/n) 2.7.1 Binomial likelihood function Kasparov only won one of six games. This is our data with \\(L(\\pi|y = 1)\\). We can calculate it for each \\(\\pi\\) value. What is more likely is that \\(pi\\) was 0.2. 2.7.2 Probability mass functions vs likelihood functions When \\(\\pi\\) is known the conditional pmf allows us to compare the probabilities of different possible value of data Y (y1, y2 ..) occuring with \\(pi\\) when Y = y is known the likelihood function allows us to to compare the relative values of \\(\\pi\\) (\\(\\pi_1, \\pi_2, etc ..\\)) 2.7.3 Normalizing constant Total probability that Kasparov would win Y = 1 game across all possible win probability of \\(\\pi\\) We apply the Law of Total Probability (the sum of all likelihood for each value of \\(\\pi\\) by the prior probailities of these \\(\\pi\\) values) \\[ f(y = 1) = L(\\pi = 0.2 | y = 1) f(\\pi = 0.2 ) + L(\\pi = 0.5 | y = 1) f(\\pi = 0.5 ) + L(\\pi = 0.8 | y = 1) f(\\pi = 0.8 ) +\\] \\[ f( y = 1) \\simeq 0.3932 * 0.1 + 0.0938 * 0.25 + 0.0015 * 0.65 \\simeq 0.0637 \\] 2.7.4 Posterior probability model We have the prior, the likelihod and the normalizing constant -&gt; Bayes Rules! \\[ posterior = \\frac{prior . likelihood}{normalizing \\quad constant} \\quad for \\in \\begin{Bmatrix} 0.2, 0 5, 0.8 \\end{Bmatrix} \\] 2.7.5 Posterior shortcut The normalizing constant, is a constant it appears in all posterior calculations. We can work with unnormalizied proabilities or we can divide each unnormalizied probability by the sum of each of them. \\[ posterior \\propto prior * likelihood \\] ## Posterior simulation # Define possible win probabilities chess &lt;- data.frame(pi = c(0.2, 0.5, 0.8)) # Define the prior model prior &lt;- c(0.10, 0.25, 0.65) # Simulate 10000 values of pi from the prior set.seed(84735) chess_sim &lt;- sample_n(chess, size = 10000, weight = prior, replace = TRUE) chess_sim &lt;- chess_sim %&gt;% mutate(y = rbinom(10000, size = 6, prob = pi)) # Focus on simulations with y = 1 win_one &lt;- chess_sim %&gt;% filter(y == 1) # Plot the posterior approximation ggplot(win_one, aes(x = pi)) + geom_bar() "],["links-shared-in-the-second-meeting.html", "2.8 Links shared in the second meeting", " 2.8 Links shared in the second meeting Bayesian probability for babies! (with cookies) : https://raw.githubusercontent.com/epimath/epid-814-materials/master/Lectures/BayesianEstimation.pdf Author: Marisa Eisenberg "],["meeting-videos-1.html", "2.9 Meeting Videos", " 2.9 Meeting Videos 2.9.1 Cohort 1 Meeting chat log LOG "],["the-beta-binomial-bayesian-model.html", "Chapter 3 The Beta-Binomial Bayesian Model", " Chapter 3 The Beta-Binomial Bayesian Model Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1.html", "3.1 SLIDE 1", " 3.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-2.html", "3.2 Meeting Videos", " 3.2 Meeting Videos 3.2.1 Cohort 1 Meeting chat log LOG "],["balance-and-sequentiality-in-bayesian-analyses.html", "Chapter 4 Balance and Sequentiality in Bayesian Analyses", " Chapter 4 Balance and Sequentiality in Bayesian Analyses Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-1.html", "4.1 SLIDE 1", " 4.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-3.html", "4.2 Meeting Videos", " 4.2 Meeting Videos 4.2.1 Cohort 1 Meeting chat log LOG "],["conjugate-families.html", "Chapter 5 Conjugate Families", " Chapter 5 Conjugate Families Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-2.html", "5.1 SLIDE 1", " 5.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-4.html", "5.2 Meeting Videos", " 5.2 Meeting Videos 5.2.1 Cohort 1 Meeting chat log LOG "],["approximating-the-posterior.html", "Chapter 6 Approximating the Posterior", " Chapter 6 Approximating the Posterior Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-3.html", "6.1 SLIDE 1", " 6.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-5.html", "6.2 Meeting Videos", " 6.2 Meeting Videos 6.2.1 Cohort 1 Meeting chat log LOG "],["mcmc-under-the-hood.html", "Chapter 7 MCMC under the Hood", " Chapter 7 MCMC under the Hood Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-4.html", "7.1 SLIDE 1", " 7.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-6.html", "7.2 Meeting Videos", " 7.2 Meeting Videos 7.2.1 Cohort 1 Meeting chat log LOG "],["posterior-inference-prediction.html", "Chapter 8 Posterior Inference &amp; Prediction", " Chapter 8 Posterior Inference &amp; Prediction Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-5.html", "8.1 SLIDE 1", " 8.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-7.html", "8.2 Meeting Videos", " 8.2 Meeting Videos 8.2.1 Cohort 1 Meeting chat log LOG "],["simple-normal-regression.html", "Chapter 9 Simple Normal Regression", " Chapter 9 Simple Normal Regression Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-6.html", "9.1 SLIDE 1", " 9.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-8.html", "9.2 Meeting Videos", " 9.2 Meeting Videos 9.2.1 Cohort 1 Meeting chat log LOG "],["evaluating-regression-models.html", "Chapter 10 Evaluating Regression Models", " Chapter 10 Evaluating Regression Models Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-7.html", "10.1 SLIDE 1", " 10.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-9.html", "10.2 Meeting Videos", " 10.2 Meeting Videos 10.2.1 Cohort 1 Meeting chat log LOG "],["extending-the-normal-regression-model.html", "Chapter 11 Extending the Normal Regression Model", " Chapter 11 Extending the Normal Regression Model Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-8.html", "11.1 SLIDE 1", " 11.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-10.html", "11.2 Meeting Videos", " 11.2 Meeting Videos 11.2.1 Cohort 1 Meeting chat log LOG "],["poisson-negative-binomial-regression.html", "Chapter 12 Poisson &amp; Negative Binomial Regression", " Chapter 12 Poisson &amp; Negative Binomial Regression Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-9.html", "12.1 SLIDE 1", " 12.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-11.html", "12.2 Meeting Videos", " 12.2 Meeting Videos 12.2.1 Cohort 1 Meeting chat log LOG "],["logistic-regression.html", "Chapter 13 Logistic Regression", " Chapter 13 Logistic Regression Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-10.html", "13.1 SLIDE 1", " 13.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-12.html", "13.2 Meeting Videos", " 13.2 Meeting Videos 13.2.1 Cohort 1 Meeting chat log LOG "],["naive-bayes-classification.html", "Chapter 14 Naive Bayes Classification", " Chapter 14 Naive Bayes Classification Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-11.html", "14.1 SLIDE 1", " 14.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-13.html", "14.2 Meeting Videos", " 14.2 Meeting Videos 14.2.1 Cohort 1 Meeting chat log LOG "],["hierarchical-models-are-exciting.html", "Chapter 15 Hierarchical Models are Exciting", " Chapter 15 Hierarchical Models are Exciting Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-12.html", "15.1 SLIDE 1", " 15.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-14.html", "15.2 Meeting Videos", " 15.2 Meeting Videos 15.2.1 Cohort 1 Meeting chat log LOG "],["normal-hierarchical-models-without-predictors.html", "Chapter 16 (Normal) Hierarchical Models without Predictors", " Chapter 16 (Normal) Hierarchical Models without Predictors Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-13.html", "16.1 SLIDE 1", " 16.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-15.html", "16.2 Meeting Videos", " 16.2 Meeting Videos 16.2.1 Cohort 1 Meeting chat log LOG "],["normal-hierarchical-models-with-predictors.html", "Chapter 17 (Normal) Hierarchical Models with Predictors", " Chapter 17 (Normal) Hierarchical Models with Predictors Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-14.html", "17.1 SLIDE 1", " 17.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-16.html", "17.2 Meeting Videos", " 17.2 Meeting Videos 17.2.1 Cohort 1 Meeting chat log LOG "],["non-normal-hierarchical-regression-classification.html", "Chapter 18 Non-Normal Hierarchical Regression &amp; Classification", " Chapter 18 Non-Normal Hierarchical Regression &amp; Classification Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-15.html", "18.1 SLIDE 1", " 18.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-17.html", "18.2 Meeting Videos", " 18.2 Meeting Videos 18.2.1 Cohort 1 Meeting chat log LOG "],["adding-more-layers.html", "Chapter 19 Adding More Layers", " Chapter 19 Adding More Layers Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-16.html", "19.1 SLIDE 1", " 19.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-18.html", "19.2 Meeting Videos", " 19.2 Meeting Videos 19.2.1 Cohort 1 Meeting chat log LOG "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
